---
published: true

layout: single

title: "[2023-05-26] CNN"

categories: [Devcourse-TIL]

tags: Autonomous_Driving CV

toc: true

toc_label : 목차

toc_sticky: true

author_profile: false

sidebar :
    nav : "docs"
---

## CNN

- Convolutional Neural Networks





### Convolution

- Input 과 Conv Kernel의 각 인덱스의 원소들이 곱해지고 그 합이 하나의 output 픽셀이 됨

<img src="https://github.com/shpark98/Projects/assets/116723552/3ad9cfb5-7ca1-4c08-9de5-dd4c70651214" alt="image" style="zoom:80%;" />





### Padding, Stride

- Padding : Input의 Boundarty 외곽에 Zero Value를 추가로 생성함
  - Padding의 값을 1을 주면 1줄의 0이 생성되고 값을 2를 주면 2줄의 0이 생성됨
  - Output 사이즈를 Input 사이즈와 동일하게 만들 때 처럼, Padding을 통해 인위적으로 Input을 늘려줌

- Stride : Conv Kenrel이 슬라이딩 윈도우 방식으로 연산할 때의 간격
  - Stride가 커질 수록 연산량은 줄어들고 Output의 size도 줄어듬
  - Stride 간격이 1일 경우에는 연산량이 많아지고 Output의 Size도 커짐

<img src="https://github.com/shpark98/Projects/assets/116723552/074f4af2-207a-4258-9c9b-a7716f7aa1ce" alt="image" style="zoom:80%;" />





### Ouput Shape

- output_height = (input_height - kernel_height + padding * 2) // stride +1

- output_width = (input_width - kernel_width + padding *2) // stride +1





### A x W = B

- kc의 값과 ic의 값은 동일함
- input과 weight의 연산을 통해 하나의 픽셀 값으로 결과가 도출되는데, 이러한 결과가 output channel 수만큼 반복해서 진행됨
- 동일한 weight들이 각 batch에서 연산됨(Weight sharing)

<img src="https://github.com/shpark98/Projects/assets/116723552/3ad516ec-135d-4e45-9c4c-e7050b743221" alt="image" style="zoom:80%;" />





### MAC

- Multiply Accumulation Operation(곱셈 누산 작업)
- 어떤 모델을 개발하거나 설계 할 때 MAC의 연산량을 통해 속도를 측정 및 예측할 수 있음

- MAC = kw * kh * kc * oc * ow * oh * b



### IM2COL & GEMM

- 컴퓨터에서 효율적으로 연산하기 위해 변환시키는 작업

- IM2COL : N차원의 데이터를 2차원으로 변환
- GEMM : General Matrix to Matrix Multiplication

<img src="https://github.com/shpark98/Projects/assets/116723552/eb16936a-6b0e-4a69-8649-dba5302c4b5c" alt="image" style="zoom:80%;" />





### Pooling

<img src="https://github.com/shpark98/Projects/assets/116723552/6d67fed0-1d7e-474e-8d8b-696ee242bc83" alt="image" style="zoom:67%;" />

- Max Pooling
  - 해당 위치에서의 최댓점을 뽑다 보니, 가장 눈에 띄는 엣지부분들을 많이 잡아냄
- Average Pooling
  - 해당 위치에서의 평균값을 뽑다 보니, smoothing(부드러워진)된 형상을 띄게 됨

<img src="https://github.com/shpark98/Projects/assets/116723552/fcea786b-58a7-4615-9ede-569ae8c99275" alt="image" style="zoom:80%;" />



### Fully Connected Layer

- 하나의 원소를 하나의 weight가 일대일로 매칭될 수 있게 만듬

![image](https://github.com/shpark98/Projects/assets/116723552/1be18c96-a77c-40cb-a18e-d99b04d26fd6)



### Activation

- sigmoid
  - x가 음수일 때 y의 값은 0으로 수렴
- tanh
  - sigmoid와 비슷하고 x가 음수일 때 y값은 음수로 수렴
- ReLU
  - x가 음수이면 0, x 가 양수일 때 x값이 나옴
- LeakyReLU
  - x가 음수일 때, 0이 아닌 0에 인접한 값으로 주고, x가 양수일 때 x 값이 나옴

<img src="https://github.com/shpark98/Projects/assets/116723552/1b7a4a2f-2c34-4ac3-8f7e-09f42e6fa581" alt="image" style="zoom:80%;" />



### Shallow CNN

- Shallow Neural Network

![image](https://github.com/shpark98/Projects/assets/116723552/5b11d41a-2934-4b9a-a3a7-ac47447a87cc)



- Backpropagation in Shallow Nerual Network

<img src="https://github.com/shpark98/Projects/assets/116723552/bba857db-e396-4687-b487-868afc733893" alt="image" style="zoom:80%;" />

<img src="https://github.com/shpark98/Projects/assets/116723552/c55883cb-593a-4781-9291-ce1d2538b791" alt="image" style="zoom:80%;" />

<img src="https://github.com/shpark98/Projects/assets/116723552/5eb4b731-fb78-4036-a2fd-1062ad9ecc2e" alt="image" style="zoom:80%;" />



### Max Pooling BackPropagation

![image](https://github.com/shpark98/Projects/assets/116723552/6e71548c-1951-4c4d-bd92-189b4019156c)



## CNN Training

### Training Model

- Feed forward & Backward를 통해 weight를 업데이트하면서 training data에 맞게 학습이됨
  - weight가 정교해지면서 iterations이 증가하면서 error 값이 줄어듬

<img src="https://github.com/shpark98/Projects/assets/116723552/0e9a0520-595e-4102-bf8e-c4acd51d6497" alt="image" style="zoom:80%;" />



### Optimizer(최적화)

- Gradient Descent(경사 하강법)
- SGD(Stochastic Gradient Descent)
  - 전체 데이터셋을 사용하지 않고, 각 학습 단계에서 무작위로 선택한 하나의 데이터 샘플에 대한 기울기를 계산하여 업데이트하는 방법
  - 데이터가 매우 큰 경우에 유용함
- Momentum 
- AdaGrad
  - 각 매개변수에 독립적인 학습률을 적용하여 학습 속도를 조정하는 방법
  - 기울기가 크게 갱신된 매개변수는 작은 학습률을 적용하고, 기울기가 작게 갱신된 매개변수는 큰 학습률을 적용하여 수렴을 개선
- RMS-prop
  - AdaGrad의 단점을 보완하기 위해 제안된 알고리즘
  - 과거 기울기의 지수 이동 평균을 사용하여 학습률을 조정하고 최근 기울기에 더 큰 가중치를 주어 더 빠른 수렴을 도모함
- Adam(Adaptive Moment Estimation)
  - 모멘텀 최적화와 RMSprop의 아이디어를 결합한 알고리즘





### Overfitting & Underfitting

- Overfitting(과대적합)
  - trainig dataset에서는 좋은 결과를 보여주지만, 새로운 dataset이 들어왔을 때 안좋은 결과를 냄
- Underfitting(과소적합)
  - trainig dataset에 대해서 아직 잘 학습이 안됨
  - 너무 적은 데이터셋을 갖고 있거나 적절하지 못한 hyperparameter이거나 학습이 아직 덜 진행되었거나 등.. 다양한 원인

<img src="https://github.com/shpark98/Projects/assets/116723552/b8083b66-60c7-4a4e-92d4-d9e5425c1c44" alt="image" style="zoom: 67%;" />





### Regularization(정규화)

- Overfitting 문제를 방지하고 모델의 일반화 성능을 향상시키기 위한 기법
- Weight가 너무 큰 값들을 가지지 않도록 만듬
  - W가 너무 큰 값을 가지게 되면 과하게 구불구불한 형태의 함수가 만들어짐
  - Regularization은 이런 모델의 복잡도를 낮추기 위한 방법이다.
- Regularization은 단순하게 cost function을 작아지는 쪽으로 학습하면 특정 가중치 값들이 커지면서 결과를 나쁘게 만들기 때문에 cost function을 바꿈
- L1 Regularization
- L2 Regularization





### Drop Out

- 과적합을 방지하기 위해 뉴런의 일부를 임의로 제거하거나 비활성화함

- 훈련 단계에서만 적용하고 테스트나 예측 단계에서는 사용하지 않음

![image](https://github.com/shpark98/Projects/assets/116723552/e8e7b719-44ed-4a43-a2d9-0a851fe58c82)



### Batch Normalization

- 훈련하는 동안 각 미니배치 데이터의 입력을 정규화함으로써 모델의 학습을 안정화시키고 성능을 향상 시킴
- input batch 데이터에 대한 평균값과 variance 값을 구해서 그것에 따른 normalization을 진행함
- 안정적인 분포의 값으로 만들어 줌

![image](https://github.com/shpark98/Projects/assets/116723552/7f7c9f79-52a3-4bd2-8719-95103922d603)
